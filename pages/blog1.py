import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
import re

st.set_page_config(layout="wide")

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-gpt2")
    model = AutoModelForCausalLM.from_pretrained("ytu-ce-cosmos/turkish-gpt2").to("cpu")
    return tokenizer, model

tokenizer, model = load_model()

user_name = st.session_state.get("user_name", "Misafir")
user_burc = st.session_state.get("zodiac", "KoÃ§")
user_gender = st.session_state.get("gender", "KadÄ±n")
user_image_path = st.session_state.get("image_path", None)

col1, col2 = st.columns([5, 1])

with col1:
    st.title(f"ğŸ‘‹ Merhaba, {user_name}!")
    st.subheader(f"âœ¨ Burcun: {user_burc} â€¢ Cinsiyet: {user_gender}")

    base_prompt = f"{user_burc} burcu {user_gender.lower()} hakkÄ±nda iÃ§ten ve Ã¶zgÃ¼n bir burÃ§ yorumu yap."
    inputs = tokenizer(base_prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=180,
            temperature=0.7,
            top_p=0.9,
            top_k=50,
            do_sample=True,
            no_repeat_ngram_size=3,
            early_stopping=True
        )

    yorum = tokenizer.decode(outputs[0], skip_special_tokens=True)
    yorum = yorum.replace(base_prompt, "").strip()

    st.markdown("### ğŸ¤– Yapay ZekÃ¢ Yorumu:")
    st.info(yorum)

with col2:
    if user_image_path and os.path.exists(user_image_path):
        st.image(user_image_path, width=120, caption=f"{user_burc} Burcu")

st.markdown("---")

st.subheader("ğŸ”® Burcuna Ã–zel Sorular Sorabilirsin")

kategori = st.selectbox("Kategori SeÃ§:", ["AÅŸk", "Kariyer", "SaÄŸlÄ±k", "Genel"])
soru = st.text_input("Sorunuz:")

if st.button("YanÄ±tla"):
    if not soru.strip():
        st.warning("LÃ¼tfen bir soru yazÄ±n.")
    else:
        soru_prompt = (
            f"{user_burc} burcu {user_gender.lower()} iÃ§in {kategori} konusunda kÄ±sa, Ã¶zgÃ¼n ve tekrarsÄ±z bir yanÄ±t ver.\n"
            f"Soru: {soru}\nYanÄ±t:"
        )

        inputs_soru = tokenizer(soru_prompt, return_tensors="pt")

        with torch.no_grad():
            outputs_soru = model.generate(
                **inputs_soru,
                max_length=150,
                temperature=0.7,
                top_p=0.9,
                top_k=40,
                do_sample=True,
                no_repeat_ngram_size=3,
                early_stopping=True
            )

        cevap_raw = tokenizer.decode(outputs_soru[0], skip_special_tokens=True)
        cevap_clean = re.split(r"YanÄ±t:\s*", cevap_raw)[-1].strip()
        cevap_clean = re.sub(r"\s+", " ", cevap_clean)
        st.success(f"ğŸ§  Yapay ZekÃ¢ YanÄ±tÄ±:\n\n{cevap_clean}")
